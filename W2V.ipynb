{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "词向量训练\n",
    "word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据预处理\n",
    "生成：idx2word,word2idx,word_counts,word_freqs,text\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "MAX_VOCAB_SIZE = 1000  # 包含一个<UNK>，实际上是构建一个999个单词的词典\n",
    "\n",
    "\n",
    "def  pre_data(data_location:str):\n",
    "    with open(data_location) as f:\n",
    "        text = f.read()  # 得到文本内容\n",
    "\n",
    "    text = text.lower().split()  # 分割成小写单词列表\n",
    "    vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
    "    # 统计并筛选词频最高的的999个，变更成key=word，value=times\n",
    "    vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values()))\n",
    "    # 把不常用的单词都编码为\"<UNK>\"，也就是999排名以后的，所有的累加\n",
    "\n",
    "    idx2word = [word for word in vocab_dict.keys()]  # 关键字排序\n",
    "    word2idx = {word: i for i, word in enumerate(idx2word)}  # 逆序\n",
    "\n",
    "    word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "    # 科学计数法，转为numpy.ndarray格式的词频统计\n",
    "\n",
    "    word_freqs = word_counts / np.sum(word_counts)  # 词频百分比\n",
    "    return text,word_freqs,word_counts,idx2word,word2idx,vocab_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#数据集的读取\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "C = 3  # context window\n",
    "K = 15  # number of negative samples\n",
    "\n",
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, text, word2idx, idx2word, word_freqs, word_counts):\n",
    "        \"\"\" text: a list of words\n",
    "            word2idx: the dictionary from word to index\n",
    "            idx2word: index to word mapping\n",
    "            word_freqs: the frequency of each word\n",
    "            word_counts: the word counts\n",
    "        \"\"\"\n",
    "\n",
    "        super()  # #通过父类初始化模型，然后重写两个方法\n",
    "\n",
    "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text]\n",
    "        # 返回单词在词典中的数字下表，把单词数字化表示。如果不在词典中，也表示为unk\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        # nn.Embedding需要传入LongTensor类型\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)#词频\n",
    "        self.word_counts = torch.Tensor(word_counts)#总数\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)  # 返回所有单词的总数，即item的总数\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的positive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        \"\"\"\n",
    "        center_words = self.text_encoded[idx]  # 取得中心词\n",
    "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))  # 先取得中心左右各C个词的索引,C在前面定义了\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]  # 为了避免索引越界，所以进行取余处理\n",
    "        pos_words = self.text_encoded[pos_indices]  # tensor(list)，获得indices下标的tensor的word,这里是2*C个\n",
    "\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "        # K 是负采样\n",
    "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\n",
    "\n",
    "        return center_words, pos_words, neg_words\n",
    "    #返回中心词，正样本词，负样本词。\n",
    "    #中心词input参数idx得到，pos_words词为周围C*2个，neg_words为词频表中按照词频概率采样的负样本"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#模型的定义\n",
    "#这里仅仅计算当前词的损失，\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)  # 词点的尺寸和嵌入向量的维度\n",
    "        # 这里的embedding就是一个线性模型，也可以换成nn.Linear\n",
    "        self.out_embed = nn.Embedding(self.embed_size, self.vocab_size)\n",
    "        # ，给一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):  # 前向传播\n",
    "        \"\"\" input_labels: center words, [batch_size]\n",
    "            pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "            neg_labels：negative words, [batch_size, (window_size * 2 * K)]\n",
    "            return: loss, [batch_size]\n",
    "        \"\"\"\n",
    "        # 得到对应标签的向量\n",
    "        input_embedding = self.in_embed(input_labels)  # [batch_size, embed_size]\n",
    "        pos_embedding = self.in_embed(pos_labels)  # [batch_size, (window * 2), embed_size]\n",
    "        neg_embedding = self.in_embed(neg_labels)  # [batch_size, (window * 2 * K), embed_size]\n",
    "\n",
    "        input_embedding = input_embedding.unsqueeze(2)  # [batch_size, embed_size, 1]\n",
    "        # 在第2维度那里加维度，(0,1,2)\n",
    "\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding)  # [batch_size, (window * 2), 1]\n",
    "        # 矩阵乘法 强制规定维度和大小相同\n",
    "        pos_dot = pos_dot.squeeze(2)  # [batch_size, (window * 2)]\n",
    "        # 删除维度\n",
    "\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding)  # [batch_size, (window * 2 * K), 1]\n",
    "        # 负样本的目的是拉开距离，所以距离越小损失越大\n",
    "        neg_dot = neg_dot.squeeze(2)  # batch_size, (window * 2 * K)]\n",
    "        # 删除维度\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)  # .sum()结果只为一个数，.sum(1)结果是一维的张量，按照1维度计算\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "\n",
    "        loss = log_pos + log_neg\n",
    "        # 共同损失\n",
    "\n",
    "        return -loss\n",
    "\n",
    "    def input_embeddings(self):\n",
    "        return self.in_embed.weight.cpu()\n",
    "\n",
    "    def out_embeddings(self):\n",
    "        return self.out_embed.weight.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#模型的训练\n",
    "import torch\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Modul import EmbeddingModel\n",
    "from WordEmbeddingDataset import WordEmbeddingDataset\n",
    "\n",
    "\n",
    "epochs = 30 #训练轮数\n",
    "\n",
    "EMBEDDING_SIZE = 100#embedding大小\n",
    "batch_size = 15000#一个批次训练的单词个数\n",
    "lr = 0.2#学习率\n",
    "\n",
    "# word_freqs = word_freqs ** (3. / 4.)  # 这是因为word2vec论文里面推荐这么做\n",
    "\n",
    "text,word_freqs,word_counts,idx2word,word2idx,vocab_dict=pre_data('data/text8.train.txt')\n",
    "\n",
    "dataset = WordEmbeddingDataset(text, word2idx, idx2word, word_freqs, word_counts)\n",
    "#加载数据集\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "#加载数据到Dataloader中，洗一下\n",
    "\n",
    "net = EmbeddingModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "#定义模型\n",
    "#net=torch.load('model/embedding_epoch_new_9_.pt')\n",
    "#加载模型\n",
    "\n",
    "net.cuda()\n",
    "#loss_min=0;\n",
    "\n",
    "print('*********Training begins*******')\n",
    "for e in range(epochs):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = torch.LongTensor(input_labels.long()).cuda()\n",
    "        pos_labels = torch.LongTensor(pos_labels.long()).cuda()\n",
    "        neg_labels = torch.LongTensor(neg_labels.long()).cuda()\n",
    "        #将数据转为LongTensor\n",
    "\n",
    "        optimizer = optim.Adam(params=net.parameters(), lr=lr, )\n",
    "        #优化器\n",
    "        optimizer.zero_grad()  # net.zero_grad()\n",
    "        #梯度清0，每个批次算的都不一样\n",
    "        loss = net.forward(input_labels, pos_labels, neg_labels).mean()\n",
    "        #返回的是loss, [batch_size]，所以要求平均损失\n",
    "        loss.backward()\n",
    "        #反向传播\n",
    "        optimizer.step()\n",
    "        #更新参数\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('epoch', e, 'iteration', i, loss.item())\n",
    "    if e==0:\n",
    "        loss_min = loss.item()\n",
    "        #第一轮的精度保存\n",
    "    if e != 0 and loss_min > loss.item():\n",
    "        #有更小的损失，则保存新的模型\n",
    "        torch.save(net, './model/epoch_{}_loss{}.pt'.format(e,loss.item()))\n",
    "        loss_min=loss.item()\n",
    "        lr = lr * 0.5\n",
    "        #学习率下降\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#把训练好的模型拿来使用\n",
    "#就是找到当前词汇近似的词。\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from WordEmbeddingDataset import WordEmbeddingDataset\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(word,vocab_dict,word2idx):\n",
    "    \"\"\"找到目标词的相似词\"\"\"\n",
    "    if word not in train_vocab_dict:#如果是词典的生词\n",
    "        word = '<UNK>'\n",
    "    index = word2idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "\n",
    "    cos_dis = np.array([cosine(e, embedding) for e in embedding_weights])\n",
    "    #计算余弦相似的词，\n",
    "    return [word2idx[i] for i in cos_dis.argsort()[:10]]#返回排序后的前10个\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 查找最近向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = torch.load('model/embedding_epoch5_.pt')\n",
    "#加载模型\n",
    "embedding_weights = net.input_embeddings().detach().numpy()\n",
    "#得到input_embeddings的维度参数，\n",
    "\n",
    "train_text,train_word_freqs,train_word_counts,train_idx2word,dev_word2idx,train_vocab_dict=pre_data('data/text8.train.txt')\n",
    "\n",
    "dev_text,dev_word_freqs,dev_word_counts,dev_idx2word,train_word2idx,dev_vocab_dict=pre_data('data/text8.dev.txt')\n",
    "\n",
    "\n",
    "print(\"****** train word*******\")\n",
    "for word in [\"apple\", \"america\", \"computer\"]:\n",
    "    print(word, find_nearest(word,train_vocab_dict,train_word2idx))\n",
    "print(\"****** dev word*******\")\n",
    "\n",
    "for word in list(dev_vocab_dict.keys())[101:105]:\n",
    "    print(word, find_nearest(word,dev_vocab_dict,dev_word2idx))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#随机取样100个样本，测试其平均误差\n",
    "train_dataset = WordEmbeddingDataset(train_text, train_word2idx,\n",
    "                                     train_idx2word, train_word_freqs,train_word_counts)\n",
    "\n",
    "\n",
    "dev_dataset = WordEmbeddingDataset(dev_text, dev_word2idx, dev_idx2word, dev_word_freqs,\n",
    "                                   dev_word_counts)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 100, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, 100, shuffle=True)\n",
    "\n",
    "\n",
    "train_loss = list()\n",
    "for i, (input_labels, pos_labels, neg_labels) in enumerate(train_dataloader):\n",
    "    input_labels = torch.LongTensor(input_labels.long()).cuda()\n",
    "    pos_labels = torch.LongTensor(pos_labels.long()).cuda()\n",
    "    neg_labels = torch.LongTensor(neg_labels.long()).cuda()\n",
    "\n",
    "    loss = net.forward(input_labels, pos_labels, neg_labels).mean()\n",
    "    train_loss.append(loss)\n",
    "    #记录平均误差\n",
    "    if i == 100:\n",
    "        break\n",
    "print(\"train_mean_loss\")\n",
    "print(sum(train_loss) / 100)\n",
    "print(\"end\")\n",
    "\n",
    "\n",
    "\n",
    "dev_loss = list()\n",
    "\n",
    "for i, (input_labels, pos_labels, neg_labels) in enumerate(dev_dataloader):\n",
    "    input_labels = torch.LongTensor(input_labels.long()).cuda()\n",
    "    pos_labels = torch.LongTensor(pos_labels.long()).cuda()\n",
    "    neg_labels = torch.LongTensor(neg_labels.long()).cuda()\n",
    "\n",
    "    loss = net.forward(input_labels, pos_labels, neg_labels).mean()\n",
    "    dev_loss.append(loss)\n",
    "    if i == 100:\n",
    "        break\n",
    "print(\"dev_mean_loss\")\n",
    "print(sum(dev_loss) / 100)\n",
    "print(\"end\")\n",
    "\n",
    "\n",
    "print(dict(net.__dict__.items()).get('_modules'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}